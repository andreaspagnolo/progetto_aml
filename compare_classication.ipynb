{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa386cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "EPOCHS = 10 # Short run to see convergence speed\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path to YOUR trained rotation model\n",
    "YOUR_MODEL_PATH = 'model_saving/fullTraining_resnet18.pth' \n",
    "ARCH_NAME = 'resnet18' # Options: resnet50, efficientnet_b0, inception_v3\n",
    "\n",
    "# Pascal VOC Classes\n",
    "VOC_CLASSES = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', \n",
    "    'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', \n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "# --- 2. DATASET HANDLING (MULTI-LABEL) ---\n",
    "class PascalVOC_Classification(datasets.VOCDetection):\n",
    "    \"\"\"\n",
    "    Wrapper to convert VOC dictionary targets into One-Hot Multi-label vectors.\n",
    "    \"\"\"\n",
    "    def __init__(self, root, year, image_set, transform=None):\n",
    "        super().__init__(root, year, image_set, download=True, transform=transform)\n",
    "        self.class_to_idx = {name: i for i, name in enumerate(VOC_CLASSES)}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "        \n",
    "        # Convert XML dictionary to One-Hot Vector (20 classes)\n",
    "        label_vector = torch.zeros(20, dtype=torch.float32)\n",
    "        \n",
    "        objects = target['annotation']['object']\n",
    "        if not isinstance(objects, list):\n",
    "            objects = [objects]\n",
    "            \n",
    "        for obj in objects:\n",
    "            class_name = obj['name']\n",
    "            if class_name in self.class_to_idx:\n",
    "                idx = self.class_to_idx[class_name]\n",
    "                label_vector[idx] = 1.0\n",
    "                \n",
    "        return img, label_vector\n",
    "\n",
    "# Transforms (Standard ImageNet stats)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load Datasets\n",
    "print(\"Preparing Data...\")\n",
    "# Ensure you have the data downloaded or set download=True\n",
    "train_dataset = PascalVOC_Classification(root='./data', year='2012', image_set='train', transform=transform)\n",
    "val_dataset = PascalVOC_Classification(root='./data', year='2012', image_set='val', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- 3. MODEL BUILDER ---\n",
    "def build_model(architecture, weights_type):\n",
    "    \"\"\"\n",
    "    weights_type: 'imagenet', 'custom', 'random'\n",
    "    \"\"\"\n",
    "    print(f\"Building {architecture} with {weights_type} weights...\")\n",
    "    \n",
    "    # 1. Initialize Base\n",
    "    if weights_type == 'imagenet':\n",
    "        model = getattr(models, architecture)(weights='DEFAULT')\n",
    "    else:\n",
    "        model = getattr(models, architecture)(weights=None)\n",
    "    \n",
    "    # 2. Load Custom Weights (if needed)\n",
    "    if weights_type == 'custom':\n",
    "        state_dict = torch.load(YOUR_MODEL_PATH, map_location='cpu')\n",
    "        \n",
    "        # Smart Filter (Same as before)\n",
    "        if list(state_dict.keys())[0].startswith('module.'):\n",
    "            state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "        \n",
    "        filtered_dict = {k: v for k, v in state_dict.items() \n",
    "                         if not ('fc' in k or 'classifier' in k or 'AuxLogits' in k)}\n",
    "        \n",
    "        model.load_state_dict(filtered_dict, strict=False)\n",
    "        print(\"-> Custom rotation weights loaded (Head ignored).\")\n",
    "\n",
    "    # 3. Replace Head for 20 Classes (Pascal VOC)\n",
    "    if 'resnet' in architecture or 'inception' in architecture:\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, 20)\n",
    "    elif 'efficientnet' in architecture:\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, 20)\n",
    "    \n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# --- 4. TRAINING LOOP ---\n",
    "def train_fine_tuning(model, name):\n",
    "    criterion = nn.BCEWithLogitsLoss() # Crucial for Multi-label\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9)\n",
    "    \n",
    "    loss_history = []\n",
    "    \n",
    "    print(f\"--- Starting Fine-Tuning: {name} ---\")\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Inception returns NamedTuple (logits, aux_logits) during train\n",
    "            if ARCH_NAME == 'inception_v3' and model.training:\n",
    "                outputs, _ = model(inputs)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        loss_history.append(epoch_loss)\n",
    "        print(f\"[{name}] Epoch {epoch+1}/{EPOCHS} - Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "    return loss_history\n",
    "\n",
    "# --- 5. EXECUTION & COMPARISON ---\n",
    "\n",
    "# A. Train Baseline (ImageNet Supervised)\n",
    "model_sup = build_model(ARCH_NAME, 'imagenet')\n",
    "hist_sup = train_fine_tuning(model_sup, \"ImageNet_Baseline\")\n",
    "\n",
    "# B. Train Your Model (Rotation Self-Supervised)\n",
    "model_self = build_model(ARCH_NAME, 'custom')\n",
    "hist_self = train_fine_tuning(model_self, \"My_Rotation_Model\")\n",
    "\n",
    "# C. (Optional) Random Init - To prove you learned SOMETHING\n",
    "model_rand = build_model(ARCH_NAME, 'random')\n",
    "hist_rand = train_fine_tuning(model_rand, \"Random_Init\")\n",
    "\n",
    "# --- 6. PLOT RESULTS ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hist_sup, label='Supervised (ImageNet)', marker='o')\n",
    "plt.plot(hist_self, label='Self-Supervised (Rotation)', marker='x')\n",
    "plt.plot(hist_rand, label='Random Init', linestyle='--') \n",
    "\n",
    "plt.title(f'Fine-Tuning Efficiency on Pascal VOC ({ARCH_NAME})')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('BCE Loss (Lower is better)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
